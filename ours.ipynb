{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from models.GAugSiam import SiameseNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pyro\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\" one layer of GCN \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.activation = activation\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        else:\n",
    "            self.b = None\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, adj, h):\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        x = h @ self.W\n",
    "        x = adj @ x\n",
    "        if self.b is not None:\n",
    "            x = x + self.b\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SAGELayer(nn.Module):\n",
    "    \"\"\" one layer of GraphSAGE with gcn aggregator \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=True):\n",
    "        super(SAGELayer, self).__init__()\n",
    "        self.linear_neigh = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        # self.linear_self = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.activation = activation\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, adj, h):\n",
    "        # using GCN aggregator\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        x = adj @ h\n",
    "        x = self.linear_neigh(x)\n",
    "        # x_neigh = self.linear_neigh(x)\n",
    "        # x_self = self.linear_self(h)\n",
    "        # x = x_neigh + x_self\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        # x = F.normalize(x, dim=1, p=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\" one layer of GAT \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.activation = activation\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_l = nn.Linear(output_dim, self.n_heads, bias=False)\n",
    "        self.attn_r = nn.Linear(output_dim, self.n_heads, bias=False)\n",
    "        self.attn_drop = nn.Dropout(p=0.6)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            else:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, adj, h):\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        x = h @ self.W # torch.Size([2708, 128])\n",
    "        # calculate attentions, both el and er are n_nodes by n_heads\n",
    "        el = self.attn_l(x)\n",
    "        er = self.attn_r(x) # torch.Size([2708, 8])\n",
    "        if isinstance(adj, torch.sparse.FloatTensor):\n",
    "            nz_indices = adj._indices()\n",
    "        else:\n",
    "            nz_indices = adj.nonzero().T\n",
    "        attn = el[nz_indices[0]] + er[nz_indices[1]] # torch.Size([13264, 8])\n",
    "        attn = F.leaky_relu(attn, negative_slope=0.2).squeeze()\n",
    "        # reconstruct adj with attentions, exp for softmax next\n",
    "        attn = torch.exp(attn) # torch.Size([13264, 8]) NOTE: torch.Size([13264]) when n_heads=1\n",
    "        if self.n_heads == 1:\n",
    "            adj_attn = torch.zeros(size=(adj.size(0), adj.size(1)), device=adj.device)\n",
    "            adj_attn.index_put_((nz_indices[0], nz_indices[1]), attn)\n",
    "        else:\n",
    "            adj_attn = torch.zeros(size=(adj.size(0), adj.size(1), self.n_heads), device=adj.device)\n",
    "            adj_attn.index_put_((nz_indices[0], nz_indices[1]), attn) # torch.Size([2708, 2708, 8])\n",
    "            adj_attn.transpose_(1, 2) # torch.Size([2708, 8, 2708])\n",
    "        # edge softmax (only softmax with non-zero entries)\n",
    "        adj_attn = F.normalize(adj_attn, p=1, dim=-1)\n",
    "        adj_attn = self.attn_drop(adj_attn)\n",
    "        # message passing\n",
    "        x = adj_attn @ x # torch.Size([2708, 8, 128])\n",
    "        if self.b is not None:\n",
    "            x = x + self.b\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if self.n_heads > 1:\n",
    "            x = x.flatten(start_dim=1)\n",
    "        return x # torch.Size([2708, 1024])\n",
    "\n",
    "\n",
    "class MultipleOptimizer():\n",
    "    \"\"\" a class that wraps multiple optimizers \"\"\"\n",
    "    def __init__(self, *op):\n",
    "        self.optimizers = op\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for op in self.optimizers:\n",
    "            op.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for op in self.optimizers:\n",
    "            op.step()\n",
    "\n",
    "    def update_lr(self, op_index, new_lr):\n",
    "        \"\"\" update the learning rate of one optimizer\n",
    "        Parameters: op_index: the index of the optimizer to update\n",
    "                    new_lr:   new learning rate for that optimizer \"\"\"\n",
    "        for param_group in self.optimizers[op_index].param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "\n",
    "class RoundNoGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.round()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g\n",
    "\n",
    "\n",
    "class CeilNoGradient(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x.ceil()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g\n",
    "\n",
    "\n",
    "def scipysp_to_pytorchsp(sp_mx):\n",
    "    \"\"\" converts scipy sparse matrix to pytorch sparse matrix \"\"\"\n",
    "    if not sp.isspmatrix_coo(sp_mx):\n",
    "        sp_mx = sp_mx.tocoo()\n",
    "    coords = np.vstack((sp_mx.row, sp_mx.col)).transpose()\n",
    "    values = sp_mx.data\n",
    "    shape = sp_mx.shape\n",
    "    pyt_sp_mx = torch.sparse.FloatTensor(torch.LongTensor(coords.T),\n",
    "                                         torch.FloatTensor(values),\n",
    "                                         torch.Size(shape))\n",
    "    return pyt_sp_mx\n",
    "\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\" GNN as node classification model \"\"\"\n",
    "    def __init__(self, dim_feats, dim_h, n_classes, n_layers, activation, dropout, gnnlayer_type='gcn'):\n",
    "        super(GNN, self).__init__()\n",
    "        heads = [1] * (n_layers + 1)\n",
    "        if gnnlayer_type == 'gcn':\n",
    "            gnnlayer = GCNLayer\n",
    "        elif gnnlayer_type == 'gsage':\n",
    "            gnnlayer = SAGELayer\n",
    "        elif gnnlayer_type == 'gat':\n",
    "            gnnlayer = GATLayer\n",
    "            if dim_feats in (50, 745, 12047): # hard coding n_heads for large graphs\n",
    "                heads = [2] * n_layers + [1]\n",
    "            else:\n",
    "                heads = [8] * n_layers + [1]\n",
    "            dim_h = int(dim_h / 8)\n",
    "            dropout = 0.6\n",
    "            activation = F.elu\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(gnnlayer(dim_feats, dim_h, heads[0], activation, 0))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(gnnlayer(dim_h*heads[i], dim_h, heads[i+1], activation, dropout))\n",
    "        # output layer\n",
    "        self.layers.append(gnnlayer(dim_h*heads[-2], n_classes, heads[-1], None, dropout))\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(adj, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    \"\"\" GAE/VGAE as edge prediction model \"\"\"\n",
    "    def __init__(self, dim_feats, dim_h, dim_z, activation, gae=False):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.gae = gae\n",
    "        self.gcn_base = GCNLayer(dim_feats, dim_h, 1, None, 0, bias=False)\n",
    "        self.gcn_mean = GCNLayer(dim_h, dim_z, 1, activation, 0, bias=False)\n",
    "        self.gcn_logstd = GCNLayer(dim_h, dim_z, 1, activation, 0, bias=False)\n",
    "        self.gcn_base2 = GCNLayer(dim_h, dim_feats, 1, None, 0, bias=False)\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        # GCN encoder\n",
    "        hidden = self.gcn_base(adj, features)\n",
    "        self.mean = self.gcn_mean(adj, hidden)\n",
    "        if self.gae:\n",
    "            # GAE (no sampling at bottleneck)\n",
    "            Z = self.mean\n",
    "        else:\n",
    "            # VGAE\n",
    "            self.logstd = self.gcn_logstd(adj, hidden)\n",
    "            gaussian_noise = torch.randn_like(self.mean)\n",
    "            sampled_Z = gaussian_noise*torch.exp(self.logstd) + self.mean\n",
    "            Z = sampled_Z\n",
    "        # inner product decoder\n",
    "        adj_logits = Z @ Z.T\n",
    "        x_hat = self.gcn_base2(Z)\n",
    "        return adj_logits, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_feats, \n",
    "                 dim_h,\n",
    "                 dim_z,\n",
    "                 n_layers, \n",
    "                 activation, \n",
    "                 dropout, \n",
    "                 dim_h2,\n",
    "                 device,\n",
    "                 gnnlayer_type='gcn',\n",
    "                 temperature=1,\n",
    "                 gae=False,\n",
    "                 ):\n",
    "        super(SiameseNet_model, self).__init__()\n",
    "        self.device = device\n",
    "        self.temperature = temperature\n",
    "        self.gnnlayer_type = gnnlayer_type\n",
    "\n",
    "        if gnnlayer_type == 'gcn':\n",
    "            gnnlayer = GCNLayer\n",
    "        elif gnnlayer_type == 'gsage':\n",
    "            gnnlayer = SAGELayer\n",
    "        elif gnnlayer_type == 'gat':\n",
    "            gnnlayer = GATLayer\n",
    "            if dim_feats in (50, 745, 12047): # hard coding n_heads for large graphs\n",
    "                heads = [2] * n_layers + [1]\n",
    "            else:\n",
    "                heads = [8] * n_layers + [1]\n",
    "            dim_h = int(dim_h / 8)\n",
    "            dropout = 0.6\n",
    "            activation = F.elu\n",
    "        self.layers1 = nn.ModuleList()\n",
    "        self.layers2 = nn.ModuleList()\n",
    "\n",
    "        # siamese submodule 1\n",
    "        # input layer\n",
    "        self.layers1.append(gnnlayer(dim_feats, dim_h, heads[0], activation, 0))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers1.append(gnnlayer(dim_h*heads[i], dim_h, heads[i+1], activation, dropout))\n",
    "        # output layer\n",
    "        self.layers1.append(gnnlayer(dim_h*heads[-2], dim_h2, heads[-1], None, dropout))\n",
    "\n",
    "        # siamese submodule 2\n",
    "        # input layer\n",
    "        self.layers2.append(gnnlayer(dim_feats, dim_h, heads[0], activation, 0))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers2.append(gnnlayer(dim_h*heads[i], dim_h, heads[i+1], activation, dropout))\n",
    "        # output layer\n",
    "        self.layers2.append(gnnlayer(dim_h*heads[-2], dim_h2, heads[-1], None, dropout))\n",
    "\n",
    "        ## score calculation\n",
    "        self.score = torch.nn.Linear(2*dim_h2, 1)\n",
    "\n",
    "        # edge prediction network\n",
    "        self.ep_net = VGAE(dim_feats, dim_h, dim_z, activation, gae=gae)\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        h = features\n",
    "        # generate graph\n",
    "        pred_adj, pred_x = self.ep_net(adj, features)\n",
    "        for layer in self.layers1:\n",
    "            h1 = layer(adj, h)\n",
    "        for layer in self.layers2:\n",
    "            h2 = layer(pred_adj, pred_x)\n",
    "        score = torch.sigmoid(self.score(torch.cat((h1, h2), dim=1)))\n",
    "        return score, pred_adj\n",
    "    \n",
    "    def normalize_adj(self, adj):\n",
    "        if self.gnnlayer_type == 'gcn':\n",
    "            # adj = adj + torch.diag(torch.ones(adj.size(0))).to(self.device)\n",
    "            adj.fill_diagonal_(1)\n",
    "            # normalize adj with A = D^{-1/2} @ A @ D^{-1/2}\n",
    "            D_norm = torch.diag(torch.pow(adj.sum(1), -0.5)).to(self.device)\n",
    "            adj = D_norm @ adj @ D_norm\n",
    "        elif self.gnnlayer_type == 'gat':\n",
    "            # adj = adj + torch.diag(torch.ones(adj.size(0))).to(self.device)\n",
    "            adj.fill_diagonal_(1)\n",
    "        elif self.gnnlayer_type == 'gsage':\n",
    "            # adj = adj + torch.diag(torch.ones(adj.size(0))).to(self.device)\n",
    "            adj.fill_diagonal_(1)\n",
    "            adj = F.normalize(adj, p=1, dim=1)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def siamese_loss(prediction, target):\n",
    "    prediction = -math.log(prediction)\n",
    "    target = -math.log(target)\n",
    "    score = (prediction-target)**2\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(object):\n",
    "    def __init__(self,\n",
    "                 adj_matrix,\n",
    "                 features,\n",
    "                 labels, \n",
    "                 tvt_nids, \n",
    "                 gnnlayer_type='gcn',\n",
    "                 cuda=-1, \n",
    "                 hidden_size=128, \n",
    "                 emb_size=32,\n",
    "                 hidden_size2=16,\n",
    "                 n_layers=1, \n",
    "                 epochs=200, \n",
    "                 seed=-1, \n",
    "                 lr=1e-2,\n",
    "                 weight_decay=5e-4, \n",
    "                 dropout=0.5, \n",
    "                 gae=False, \n",
    "                 temperature=0.2, \n",
    "                 log=True, \n",
    "                 name='debug', \n",
    "                 warmup=3,\n",
    "                 feat_norm='row'):\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.n_epochs = epochs\n",
    "        self.gae = gae\n",
    "        self.warmup = warmup\n",
    "        self.feat_norm = feat_norm\n",
    "        # create a logger, logs are saved to GAug-[name].log when name is not None\n",
    "        if log:\n",
    "            self.logger = self.get_logger(name)\n",
    "        else:\n",
    "            # disable logger if wanted\n",
    "            # logging.disable(logging.CRITICAL)\n",
    "            self.logger = logging.getLogger()\n",
    "        # config device (force device to cpu when cuda is not available)\n",
    "        if not torch.cuda.is_available():\n",
    "            cuda = -1\n",
    "        self.device = torch.device(f'cuda:{cuda}' if cuda>=0 else 'cpu')\n",
    "        # log all parameters to keep record\n",
    "        all_vars = locals()\n",
    "        self.log_parameters(all_vars)\n",
    "        # fix random seeds if needed\n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # load data\n",
    "        self.load_data(adj_matrix, features, labels, tvt_nids, gnnlayer_type)\n",
    "        # setup the model\n",
    "        self.model = SiameseNet_model(\n",
    "                 self.features.size(1),\n",
    "                 hidden_size,\n",
    "                 emb_size,\n",
    "                 n_layers,\n",
    "                 F.relu, \n",
    "                 dropout, \n",
    "                 hidden_size2,\n",
    "                 self.device,\n",
    "                 gnnlayer_type='gcn',\n",
    "                 temperature=temperature,\n",
    "                 gae=gae)\n",
    "\n",
    "    def load_data(self, adj_matrix, features, labels, tvt_nids, gnnlayer_type):\n",
    "        \"\"\" preprocess data \"\"\"\n",
    "        # features (torch.FloatTensor)\n",
    "        if isinstance(features, torch.FloatTensor):\n",
    "            self.features = features\n",
    "        else:\n",
    "            self.features = torch.FloatTensor(features)\n",
    "        # normalize feature matrix if needed\n",
    "        if self.feat_norm == 'row':\n",
    "            self.features = F.normalize(self.features, p=1, dim=1)\n",
    "        elif self.feat_norm == 'col':\n",
    "            self.features = self.col_normalization(self.features)\n",
    "        # original adj_matrix for training vgae (torch.FloatTensor)\n",
    "        assert sp.issparse(adj_matrix)\n",
    "        if not isinstance(adj_matrix, sp.coo_matrix):\n",
    "            adj_matrix = sp.coo_matrix(adj_matrix)\n",
    "        adj_matrix.setdiag(1)\n",
    "        self.adj_orig = scipysp_to_pytorchsp(adj_matrix).to_dense()\n",
    "        # normalized adj_matrix used as input for ep_net (torch.sparse.FloatTensor)\n",
    "        degrees = np.array(adj_matrix.sum(1))\n",
    "        degree_mat_inv_sqrt = sp.diags(np.power(degrees, -0.5).flatten())\n",
    "        adj_norm = degree_mat_inv_sqrt @ adj_matrix @ degree_mat_inv_sqrt\n",
    "        self.adj_norm = scipysp_to_pytorchsp(adj_norm)\n",
    "        # adj_matrix used as input for nc_net (torch.sparse.FloatTensor)\n",
    "        if gnnlayer_type == 'gcn':\n",
    "            self.adj = scipysp_to_pytorchsp(adj_norm)\n",
    "        elif gnnlayer_type == 'gsage':\n",
    "            adj_matrix_noselfloop = sp.coo_matrix(adj_matrix)\n",
    "            # adj_matrix_noselfloop.setdiag(0)\n",
    "            # adj_matrix_noselfloop.eliminate_zeros()\n",
    "            adj_matrix_noselfloop = sp.coo_matrix(adj_matrix_noselfloop / adj_matrix_noselfloop.sum(1))\n",
    "            self.adj = scipysp_to_pytorchsp(adj_matrix_noselfloop)\n",
    "        elif gnnlayer_type == 'gat':\n",
    "            # self.adj = scipysp_to_pytorchsp(adj_matrix)\n",
    "            self.adj = torch.FloatTensor(adj_matrix.todense())\n",
    "        # labels (torch.LongTensor) and train/validation/test nids (np.ndarray)\n",
    "        if len(labels.shape) == 2:\n",
    "            labels = torch.FloatTensor(labels)\n",
    "        else:\n",
    "            labels = torch.LongTensor(labels)\n",
    "        self.labels = labels\n",
    "        self.train_nid = tvt_nids[0]\n",
    "        self.val_nid = tvt_nids[1]\n",
    "        self.test_nid = tvt_nids[2]\n",
    "        # number of classes\n",
    "        if len(self.labels.size()) == 1:\n",
    "            self.out_size = len(torch.unique(self.labels))\n",
    "        else:\n",
    "            self.out_size = labels.size(1)\n",
    "        # sample the edges to evaluate edge prediction results\n",
    "        # sample 10% (1% for large graph) of the edges and the same number of no-edges\n",
    "        if labels.size(0) > 5000:\n",
    "            edge_frac = 0.01\n",
    "        else:\n",
    "            edge_frac = 0.1\n",
    "        adj_matrix = sp.csr_matrix(adj_matrix)\n",
    "        n_edges_sample = int(edge_frac * adj_matrix.nnz / 2)\n",
    "        # sample negative edges\n",
    "        neg_edges = []\n",
    "        added_edges = set()\n",
    "        while len(neg_edges) < n_edges_sample:\n",
    "            i = np.random.randint(0, adj_matrix.shape[0])\n",
    "            j = np.random.randint(0, adj_matrix.shape[0])\n",
    "            if i == j:\n",
    "                continue\n",
    "            if adj_matrix[i, j] > 0:\n",
    "                continue\n",
    "            if (i, j) in added_edges:\n",
    "                continue\n",
    "            neg_edges.append([i, j])\n",
    "            added_edges.add((i, j))\n",
    "            added_edges.add((j, i))\n",
    "        neg_edges = np.asarray(neg_edges)\n",
    "        # sample positive edges\n",
    "        nz_upper = np.array(sp.triu(adj_matrix, k=1).nonzero()).T\n",
    "        np.random.shuffle(nz_upper)\n",
    "        pos_edges = nz_upper[:n_edges_sample]\n",
    "        self.val_edges = np.concatenate((pos_edges, neg_edges), axis=0)\n",
    "        self.edge_labels = np.array([1]*n_edges_sample + [0]*n_edges_sample)\n",
    "\n",
    "    def pretrain_ep_net(self, model, adj, features, adj_orig, norm_w, pos_weight, n_epochs):\n",
    "        \"\"\" pretrain the edge prediction network \"\"\"\n",
    "        optimizer = torch.optim.Adam(model.ep_net.parameters(),\n",
    "                                     lr=self.lr)\n",
    "        model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            adj_logits = model.ep_net(adj, features)\n",
    "            loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, adj_orig, pos_weight=pos_weight)\n",
    "            if not self.gae:\n",
    "                mu = model.ep_net.mean\n",
    "                lgstd = model.ep_net.logstd\n",
    "                kl_divergence = 0.5/adj_logits.size(0) * (1 + 2*lgstd - mu**2 - torch.exp(2*lgstd)).sum(1).mean()\n",
    "                loss -= kl_divergence\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            adj_pred = torch.sigmoid(adj_logits.detach()).cpu()\n",
    "            ep_auc, ep_ap = self.eval_edge_pred(adj_pred, self.val_edges, self.edge_labels)\n",
    "            self.logger.info('EPNet pretrain, Epoch [{:3}/{}]: loss {:.4f}, auc {:.4f}, ap {:.4f}'\n",
    "                        .format(epoch+1, n_epochs, loss.item(), ep_auc, ep_ap))\n",
    "    \n",
    "    def log_parameters(self, all_vars):\n",
    "        \"\"\" log all variables in the input dict excluding the following ones \"\"\"\n",
    "        del all_vars['self']\n",
    "        del all_vars['adj_matrix']\n",
    "        del all_vars['features']\n",
    "        del all_vars['labels']\n",
    "        del all_vars['tvt_nids']\n",
    "        self.logger.info(f'Parameters: {all_vars}')\n",
    "\n",
    "    def fit(self, pretrain_ep=200):\n",
    "        \"\"\" train the model \"\"\"\n",
    "        # move data to device\n",
    "        adj_norm = self.adj_norm.to(self.device)\n",
    "        adj = self.adj.to(self.device)\n",
    "        features = self.features.to(self.device)\n",
    "        \n",
    "        adj_orig = self.adj_orig.to(self.device)\n",
    "        model = self.model.to(self.device)\n",
    "        # weights for log_lik loss when training EP net\n",
    "        adj_t = self.adj_orig\n",
    "        norm_w = adj_t.shape[0]**2 / float((adj_t.shape[0]**2 - adj_t.sum()) * 2)\n",
    "        pos_weight = torch.FloatTensor([float(adj_t.shape[0]**2 - adj_t.sum()) / adj_t.sum()]).to(self.device)\n",
    "        # pretrain VGAE if needed\n",
    "        if pretrain_ep:\n",
    "            self.pretrain_ep_net(model, adj_norm, features, adj_orig, norm_w, pos_weight, pretrain_ep)\n",
    "        # optimizers\n",
    "        optims = MultipleOptimizer(torch.optim.Adam(model.ep_net.parameters(),\n",
    "                                                    lr=self.lr),\n",
    "                                   torch.optim.Adam(model.nc_net.parameters(),\n",
    "                                                    lr=self.lr,\n",
    "                                                    weight_decay=self.weight_decay))\n",
    "        # get the learning rate schedule for the optimizer of ep_net if needed\n",
    "        if self.warmup:\n",
    "            ep_lr_schedule = self.get_lr_schedule_by_sigmoid(self.n_epochs, self.lr, self.warmup)\n",
    "\n",
    "        # keep record of the best validation accuracy for early stopping\n",
    "        best_test_score = 0.\n",
    "        patience_step = 0\n",
    "        # train model\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # update the learning rate for ep_net if needed\n",
    "            if self.warmup:\n",
    "                optims.update_lr(0, ep_lr_schedule[epoch])\n",
    "\n",
    "            model.train()\n",
    "            score, adj_logits = model(adj, features)\n",
    "            # loss function for Siamese model\n",
    "            siam_loss = siamese_loss(score, torch.tensor(1))\n",
    "            ep_loss = norm_w * F.binary_cross_entropy_with_logits(adj_logits, adj_orig, pos_weight=pos_weight)\n",
    "            loss += self.beta * ep_loss\n",
    "            optims.zero_grad()\n",
    "            loss.backward()\n",
    "            optims.step()\n",
    "            # validate (without dropout)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_score = model(adj, features)\n",
    "            if test_score > best_test_score:\n",
    "                best_test_score = test_score\n",
    "                self.logger.info('Epoch [{:3}/{}]: ep loss {:.4f}, nc loss {:.4f}, ep auc: {:.4f}, ep ap {:.4f}, val acc {:.4f}, test acc {:.4f}'\n",
    "                            .format(epoch+1, self.n_epochs, ep_loss.item(), siam_loss.item(), test_score))\n",
    "                patience_step = 0\n",
    "\n",
    "        # release RAM and GPU memory\n",
    "        del adj, features, labels, adj_orig\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return test_score\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def eval_edge_pred(adj_pred, val_edges, edge_labels):\n",
    "        logits = adj_pred[val_edges.T]\n",
    "        logits = np.nan_to_num(logits)\n",
    "        roc_auc = roc_auc_score(edge_labels, logits)\n",
    "        ap_score = average_precision_score(edge_labels, logits)\n",
    "        return roc_auc, ap_score\n",
    "\n",
    "    @staticmethod\n",
    "    def col_normalization(features):\n",
    "        \"\"\" column normalization for feature matrix \"\"\"\n",
    "        features = features.numpy()\n",
    "        m = features.mean(axis=0)\n",
    "        s = features.std(axis=0, ddof=0, keepdims=True) + 1e-12\n",
    "        features -= m\n",
    "        features /= s\n",
    "        return torch.FloatTensor(features)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_logger(name):\n",
    "        \"\"\" create a nice logger \"\"\"\n",
    "        logger = logging.getLogger(name)\n",
    "        # clear handlers if they were created in other runs\n",
    "        if (logger.hasHandlers()):\n",
    "            logger.handlers.clear()\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "        # create console handler add add to logger\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        # create file handler add add to logger when name is not None\n",
    "        if name is not None:\n",
    "            fh = logging.FileHandler(f'GAug-{name}.log')\n",
    "            fh.setFormatter(formatter)\n",
    "            fh.setLevel(logging.DEBUG)\n",
    "            logger.addHandler(fh)\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载（以Cora为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1139144/3156195896.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  adj_orig = pickle.load(open(f'data/graphs/cora_adj.pkl', 'rb'))\n",
      "/tmp/ipykernel_1139144/3156195896.py:3: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  features = pickle.load(open(f'data/graphs/cora_features.pkl', 'rb'))\n"
     ]
    }
   ],
   "source": [
    "tvt_nids = pickle.load(open(f'data/graphs/cora_tvt_nids.pkl', 'rb'))\n",
    "adj_orig = pickle.load(open(f'data/graphs/cora_adj.pkl', 'rb'))\n",
    "features = pickle.load(open(f'data/graphs/cora_features.pkl', 'rb'))\n",
    "labels = pickle.load(open(f'data/graphs/cora_labels.pkl', 'rb'))\n",
    "if sp.issparse(features):\n",
    "    features = torch.FloatTensor(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 04:28:09,397 - Parameters: {'gnnlayer_type': 'gcn', 'cuda': -1, 'hidden_size': 128, 'emb_size': 32, 'hidden_size2': 16, 'n_layers': 1, 'epochs': 200, 'seed': -1, 'lr': 0.01, 'weight_decay': 0.0005, 'dropout': 0.5, 'gae': False, 'temperature': 0.2, 'log': True, 'name': 'debug', 'warmup': 3, 'feat_norm': 'row'}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'heads' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/Min/GAug/ours.ipynb 单元格 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model \u001b[39m=\u001b[39m SiameseNet(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         adj_orig,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         features,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         labels, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         tvt_nids, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         gnnlayer_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgcn\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         cuda\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         hidden_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         emb_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         hidden_size2\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         n_layers\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         seed\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         lr\u001b[39m=\u001b[39;49m\u001b[39m1e-2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         weight_decay\u001b[39m=\u001b[39;49m\u001b[39m5e-4\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         dropout\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         gae\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdebug\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         warmup\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         feat_norm\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrow\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(pretrain_ep\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n",
      "\u001b[1;32m/users/Min/GAug/ours.ipynb 单元格 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_data(adj_matrix, features, labels, tvt_nids, gnnlayer_type)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# setup the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m SiameseNet_model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m          hidden_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m          emb_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m          n_layers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m          F\u001b[39m.\u001b[39;49mrelu, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m          dropout, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m          hidden_size2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m          gnnlayer_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgcn\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m          temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m          gae\u001b[39m=\u001b[39;49mgae)\n",
      "\u001b[1;32m/users/Min/GAug/ours.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# siamese submodule 1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# input layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers1\u001b[39m.\u001b[39mappend(gnnlayer(dim_feats, dim_h, heads[\u001b[39m0\u001b[39m], activation, \u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# hidden layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22736d323230752d31307331303534312e776973632e636c6f75646c61622e7573222c2275736572223a224d696e227d/users/Min/GAug/ours.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'heads' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    model = SiameseNet(\n",
    "        adj_orig,\n",
    "        features,\n",
    "        labels, \n",
    "        tvt_nids, \n",
    "        gnnlayer_type='gcn',\n",
    "        cuda=-1, \n",
    "        hidden_size=128, \n",
    "        emb_size=32,\n",
    "        hidden_size2=16,\n",
    "        n_layers=1, \n",
    "        epochs=200, \n",
    "        seed=-1, \n",
    "        lr=1e-2,\n",
    "        weight_decay=5e-4, \n",
    "        dropout=0.5, \n",
    "        gae=False, \n",
    "        temperature=0.2, \n",
    "        log=True, \n",
    "        name='debug', \n",
    "        warmup=3,\n",
    "        feat_norm='row')\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    score = model.fit(pretrain_ep=200)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
